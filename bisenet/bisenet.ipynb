{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "# Build data loader\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base Convolution block\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
    "        super().__init__()\n",
    "        # Head of block is a convulution layer\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        )\n",
    "        # After conv layer is the batch noarmalization layer \n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Tail of this block is the ReLU function \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Main forward of this block \n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm(x)\n",
    "        return self.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Spatial Path with 3 layers of ConvBlock \n",
    "\n",
    "class SpatialPath(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBlock(in_channels=3, out_channels=64)\n",
    "        self.conv2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return self.conv3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial_path = SpatialPath()\n",
    "# spatial_path = spatial_path.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "\n",
    "# summary(spatial_path, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Refinement Module \n",
    "\n",
    "class AttentionRefinementModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        # Apply Global Average Pooling\n",
    "        x = self.avg_pool(x_input)\n",
    "        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        # Channel of x_input and x must be same \n",
    "        return torch.mul(x_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arm_module = AttentionRefinementModule(in_channels=3, out_channels=3)\n",
    "# arm_module = arm_module.cuda()\n",
    "\n",
    "# summary(arm_module, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feature Fusion Module \n",
    "class FeatureFusionModule(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.conv_block = ConvBlock(in_channels=in_channels, out_channels=num_classes, stride=1)\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_classes, out_channels=num_classes, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=num_classes, out_channels=num_classes, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "            \n",
    "    def forward(self, x_input_1, x_input_2):\n",
    "        x = torch.cat((x_input_1, x_input_2), dim=1)\n",
    "        assert self.in_channels == x.size(1), 'in_channels of ConvBlock should be {}'.format(x.size(1))\n",
    "        feature = self.conv_block(x)\n",
    "        \n",
    "        # Apply above branch in feature \n",
    "        x = self.avg_pool(feature)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.sigmoid(self.conv2(x))\n",
    "        \n",
    "        # Multipy feature and x \n",
    "        x = torch.mul(feature, x)\n",
    "        \n",
    "        # Combine feature and x\n",
    "        return torch.add(feature, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build context path \n",
    "class ContextPath(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.features = models.resnet18(pretrained=pretrained)\n",
    "        self.conv1 = self.features.conv1\n",
    "        self.bn1 = self.features.bn1\n",
    "        self.relu = self.features.relu\n",
    "        self.max_pool = self.features.maxpool\n",
    "        self.layer1 = self.features.layer1\n",
    "        self.layer2 = self.features.layer2\n",
    "        self.layer3 = self.features.layer3\n",
    "        self.layer4 = self.features.layer4\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        # Get feature from lightweight backbone network\n",
    "        x = self.conv1(x_input)\n",
    "        x = self.relu(self.bn1(x))\n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        # Downsample 1/4\n",
    "        feature1 = self.layer1(x)\n",
    "        \n",
    "        # Downsample 1/8\n",
    "        feature2 = self.layer2(feature1)\n",
    "        \n",
    "        # Downsample 1/16\n",
    "        feature3 = self.layer3(feature2)\n",
    "        \n",
    "        # Downsample 1/32\n",
    "        feature4 = self.layer4(feature3)\n",
    "        \n",
    "        # Build tail with global averange pooling \n",
    "        tail = self.avg_pool(feature4)\n",
    "        return feature3, feature4, tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp = ContextPath()\n",
    "# cp = cp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BiSeNet \n",
    "\n",
    "class BiSeNet(nn.Module):\n",
    "    def __init__(self, num_classes, training=True):\n",
    "        super().__init__()\n",
    "        self.training = training\n",
    "        self.spatial_path = SpatialPath()\n",
    "        self.context_path = ContextPath()\n",
    "        self.arm1 = AttentionRefinementModule(in_channels=256, out_channels=256)\n",
    "        self.arm2 = AttentionRefinementModule(in_channels=512, out_channels=512)\n",
    "        \n",
    "        # Supervision for calculate loss \n",
    "        self.supervision1 = nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=1)\n",
    "        self.supervision2 = nn.Conv2d(in_channels=512, out_channels=num_classes, kernel_size=1)\n",
    "        \n",
    "        # Feature fusion module \n",
    "        self.ffm = FeatureFusionModule(num_classes=num_classes, in_channels=1024)\n",
    "        \n",
    "        # Final convolution \n",
    "        self.conv = nn.Conv2d(in_channels=num_classes, out_channels=num_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        # Spatial path output\n",
    "        sp_out = self.spatial_path(x_input)\n",
    "        \n",
    "        # Context path output\n",
    "        feature1, feature2, tail = self.context_path(x_input)\n",
    "        \n",
    "        # apply attention refinement module \n",
    "        feature1, feature2 = self.arm1(feature1), self.arm2(feature2)\n",
    "        \n",
    "        # Combine output of lightweight model with tail \n",
    "        feature2 = torch.mul(feature2, tail)\n",
    "        \n",
    "        # Up sampling \n",
    "        size2d_out = sp_out.size()[-2:]\n",
    "        feature1 = F.interpolate(feature1, size=size2d_out, mode='bilinear')\n",
    "        feature2 = F.interpolate(feature2, size=size2d_out, mode='bilinear')\n",
    "        context_out = torch.cat((feature1, feature2), dim=1)\n",
    "        \n",
    "        # Apply Feature Fusion Module \n",
    "        combine_feature = self.ffm(sp_out, context_out)\n",
    "        \n",
    "        # Up sampling \n",
    "        bisenet_out = F.interpolate(combine_feature, scale_factor=8, mode='bilinear')\n",
    "        bisenet_out = self.conv(bisenet_out)\n",
    "        \n",
    "        # When training model \n",
    "        if self.training is True:\n",
    "            feature1_sup = self.supervision1(feature1)\n",
    "            feature2_sup = self.supervision2(feature2)\n",
    "            feature1_sup = F.interpolate(feature1_sup, size=x_input.size()[-2:], mode='bilinear')\n",
    "            feature2_sup = F.interpolate(feature2_sup, size=x_input.size()[-2:], mode='bilinear')        \n",
    "            return bisenet_out, feature1_sup, feature2_sup\n",
    "        return bisenet_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bisenet = BiSeNet(num_classes=12, training=True)\n",
    "# bisenet = bisenet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output, output_sup1, output_sup2 = bisenet(torch.rand((8, 3, 720, 960)).cuda())\n",
    "\n",
    "# output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(bisenet, (3, 720, 960))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamVidDataset(Dataset):\n",
    "    # Default encoding for pixel value, class name, and class color\n",
    "    color_encoding = [\n",
    "        ('sky', (128, 128, 128)),\n",
    "        ('building', (128, 0, 0)),\n",
    "        ('pole', (192, 192, 128)),\n",
    "        ('road', (128, 64, 128)),\n",
    "        ('side_walk', (0, 0, 192)),\n",
    "        ('tree', (128, 128, 0)),\n",
    "        ('sign_symbol', (192, 128, 128)),\n",
    "        ('fence', (64, 64, 128)),\n",
    "        ('car', (64, 0, 128)),\n",
    "        ('pedestrian', (64, 64, 0)),\n",
    "        ('bicyclist', (0, 128, 192)),\n",
    "        ('unlabeled', (0, 0, 0))\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, mode='train', num_classes=12):\n",
    "        self.mode = mode\n",
    "        self.num_classes = num_classes\n",
    "        # Normailization \n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "        \n",
    "        self.DATA_PATH = os.path.join(os.getcwd(), 'CamVid/')\n",
    "\n",
    "        self.train_path, self.val_path, self.test_path = [os.path.join(self.DATA_PATH, x) for x in ['train', 'val', 'test']]\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self.data_files = self.get_files(self.train_path)\n",
    "            self.label_files = [self.get_label_file(f, 'train', 'train_labels') for f in self.data_files]\n",
    "        elif self.mode == 'val':\n",
    "            self.data_files = self.get_files(self.val_path)\n",
    "            self.label_files = [self.get_label_file(f, 'val', 'val_labels') for f in self.data_files]\n",
    "        elif self.mode == 'test':\n",
    "            self.data_files = self.get_files(self.test_path)\n",
    "            self.label_files = [self.get_label_file(f, 'test', 'test_labels') for f in self.data_files]\n",
    "        else:\n",
    "            raise RuntimeError(\"Unexpected dataset mode. \"\n",
    "                               \"Supported modes are: train, val and test\")\n",
    "    \n",
    "    def get_files(self, data_folder):\n",
    "        \"\"\"\n",
    "            Return all files in folder with extension \n",
    "        \"\"\"\n",
    "        return glob(\"{}/*.{}\".format(data_folder, 'png'))\n",
    "    \n",
    "    def get_label_file(self, data_path, data_dir, label_dir):\n",
    "        \"\"\"\n",
    "            Return label path for data_path file \n",
    "        \"\"\"\n",
    "        data_path = data_path.replace(data_dir, label_dir)\n",
    "        fname, ext = data_path.split('.')\n",
    "        return \"{}_L.{}\".format(fname, ext)\n",
    "\n",
    "    def image_loader(self, data_path, label_path):\n",
    "        data = Image.open(data_path)\n",
    "        label = Image.open(label_path)\n",
    "\n",
    "        return data, label\n",
    "    \n",
    "    def label_for_cross_entropy(self, label):\n",
    "        \"\"\"\n",
    "            Convert label image to matrix classes for apply cross entropy loss. \n",
    "            Return semantic index, label in enumemap of H x W x class\n",
    "        \"\"\"\n",
    "        semantic_map = np.zeros(label.shape[:-1])\n",
    "        # Fill all value with class 12 - default for all pixels\n",
    "        semantic_map.fill(self.num_classes - 1)\n",
    "        # Fill the pixel with correct class \n",
    "        for class_index, color_info in enumerate(self.color_encoding):\n",
    "            color = color_info[1]\n",
    "            equality = np.equal(label, color)\n",
    "            class_map = np.all(equality, axis=-1)\n",
    "            semantic_map[class_map] = class_index\n",
    "        return semantic_map\n",
    "            \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "            - index (``int``): index of the item in the dataset\n",
    "            Returns:\n",
    "            A tuple of ``PIL.Image`` (image, label) where label is the ground-truth\n",
    "            of the image.\n",
    "        \"\"\"\n",
    "        \n",
    "        data_path, label_path = self.data_files[index], self.label_files[index]\n",
    "        img, label = self.image_loader(data_path, label_path)\n",
    "\n",
    "        # Apply normalization in img\n",
    "        img = self.normalize(img)\n",
    "        # Convert label for cross entropy\n",
    "        label = np.array(label)\n",
    "        label = self.label_for_cross_entropy(label)\n",
    "        label = torch.from_numpy(label).long()\n",
    "            \n",
    "        return img, label \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            Return len of dataset \n",
    "        \"\"\"\n",
    "        return len(self.data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([720, 960])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camvid = CamVidDataset()\n",
    "\n",
    "feat, label = camvid.__getitem__(1)\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "\n",
    "EPOCHS = 200 \n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 8\n",
    "CHECKPOINT_STEP = 2\n",
    "VALIDATE_STEP = 1\n",
    "\n",
    "NUM_CLASSES = 12\n",
    "\n",
    "model = BiSeNet(num_classes=NUM_CLASSES, training=True)\n",
    "model = model.cuda()\n",
    "\n",
    "# Dataloader for train\n",
    "dataset_train = CamVidDataset(mode='train')\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Dataloader for validate\n",
    "dataset_val = CamVidDataset(mode='val')\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix  \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_one_hot(image):\n",
    "    # Convert output of model to predicted class \n",
    "    image = image.permute(1, 2, 0)\n",
    "    x = torch.argmax(image, dim=-1)\n",
    "    return x\n",
    "\n",
    "def compute_accuracy(pred, label):\n",
    "    pred = pred.flatten()\n",
    "    label = label.flatten()\n",
    "    total = len(label)\n",
    "    count = 0.0\n",
    "    for i in range(total):\n",
    "        if pred[i] == label[i]:\n",
    "            count = count + 1.0\n",
    "    return float(count) / float(total)\n",
    "\n",
    "def fast_hist(a, b, n):\n",
    "    k = (a >= 0) & (a < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n",
    "\n",
    "\n",
    "def per_class_iu(hist):\n",
    "    epsilon = 1e-5\n",
    "    return (np.diag(hist) + epsilon) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)\n",
    "\n",
    "def val(model, dataloader):\n",
    "    accuracy_arr = []\n",
    "    \n",
    "    hist = np.zeros((NUM_CLASSES, NUM_CLASSES))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        print('Starting validate')\n",
    "        \n",
    "        for i, (val_data, val_label) in enumerate(dataloader):\n",
    "            val_data = val_data.cuda()\n",
    "            # The output of model is (1, num_classes, W, H) => (num_classes, W, H)\n",
    "            val_output = model(val_data).squeeze()\n",
    "            # Convert the (num_classes, W, H) => (W, H) with one hot decoder \n",
    "            val_output = reverse_one_hot(val_output)\n",
    "            val_output = np.array(val_output.cpu())\n",
    "            # Process label. Convert to (W, H) image \n",
    "            val_label = val_label.squeeze()\n",
    "            val_label = np.array(val_label.cpu())\n",
    "            # Compute accuracy and iou\n",
    "            accuracy = compute_accuracy(val_output, val_label)\n",
    "            hist += fast_hist(val_label.flatten(), val_output.flatten(), NUM_CLASSES)\n",
    "            # Append for calculate \n",
    "            accuracy_arr.append(accuracy)\n",
    "        miou_list = per_class_iu(hist)[:-1]\n",
    "        mean_accuracy, mean_iou = np.mean(accuracy_arr), np.mean(miou_list)\n",
    "        print('Mean accuracy: {} Mean IoU: {}'.format(mean_accuracy, mean_iou))\n",
    "        return mean_accuracy, mean_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('checkpoints/best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop for training \n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# max_miou = 0\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     model.train()\n",
    "#     tq = tqdm(total=len(dataloader_train) * BATCH_SIZE)\n",
    "#     tq.set_description('Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "\n",
    "#     loss_record = []\n",
    "    \n",
    "#     for i, (data, label) in enumerate(dataloader_train):\n",
    "#         data = data.cuda()\n",
    "#         label = label.cuda()\n",
    "#         output, output_sup1, output_sup2 = model(data)\n",
    "#         loss1 = loss_func(output, label)\n",
    "#         loss2 = loss_func(output_sup1, label)\n",
    "#         loss3 = loss_func(output_sup2, label)\n",
    "\n",
    "#         # Combine 3 loss   \n",
    "#         loss = loss1 + loss2 + loss3\n",
    "#         tq.update(BATCH_SIZE)\n",
    "#         tq.set_postfix(loss='%.6f' % loss)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         loss_record.append(loss.item())\n",
    "#     tq.close()\n",
    "#     loss_train_mean = np.mean(loss_record)\n",
    "#     print('loss for train : %f' % (loss_train_mean))\n",
    "\n",
    "#     # Save checkpoint \n",
    "#     if epoch % CHECKPOINT_STEP == 0:\n",
    "#         torch.save(model.state_dict(), 'checkpoints/lastest_model.pth')\n",
    "        \n",
    "#     # Validate save best model \n",
    "#     # Save checkpoint \n",
    "#     if epoch % VALIDATE_STEP == 0:\n",
    "#         _, mean_iou = val(model, dataloader_val)\n",
    "#         if mean_iou > max_miou:\n",
    "#             max_miou = mean_iou\n",
    "#             print('Save best model with mIoU = {}'.format(mean_iou))\n",
    "#             torch.save(model.state_dict(), 'checkpoints/best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vibloteam/timit_env/lib/python3.5/site-packages/torch/nn/functional.py:2479: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.8569447052336671 Mean IoU: 0.5705247647870512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8569447052336671, 0.5705247647870512)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataloader for testing\n",
    "dataset_test = CamVidDataset(mode='test')\n",
    "dataloader_test = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colour_code_segmentation(image, label_values):\n",
    "    w = image.shape[0]\n",
    "    h = image.shape[1]\n",
    "    x = np.zeros([w,h,3], dtype=np.uint8)\n",
    "    colour_codes = label_values\n",
    "    \n",
    "    for i in range(0, w):\n",
    "        for j in range(0, h):\n",
    "            x[i, j, :] = colour_codes[int(image[i, j])]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_values(dataset):\n",
    "    # Input is dataset instance define above \n",
    "    return [v[1] for v in dataset_test.color_encoding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show(img):\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(img, interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "def to_rgb(image):\n",
    "    test_label_vis = colour_code_segmentation(test_label, label_values)\n",
    "    img = Image.fromarray(test_label_vis, 'RGB')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_image(model, dataset_test, index):\n",
    "    test_image, test_label = dataset_test.__getitem__(index)\n",
    "    test_image, test_label = test_image.cuda(), test_label.cuda()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    test_output = model(test_image.unsqueeze(0))\n",
    "    # Convert the (num_classes, W, H) => (W, H) with one hot decoder \n",
    "    test_output = reverse_one_hot(test_output.squeeze(0))\n",
    "    test_output = np.array(test_output.cpu())\n",
    "    # Process label. Convert to (W, H) image \n",
    "    test_label = test_label.squeeze()\n",
    "    test_label = np.array(test_label.cpu())\n",
    "    return test_label, test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vibloteam/timit_env/lib/python3.5/site-packages/torch/nn/functional.py:2479: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAD8CAYAAADzEfagAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnW3MJtV5339XweCuvd1dHHe7L6TggBxZbo3dFYvltJqC3WJief3BoaCoxpRqLRW7pErlXVqpPanyYV1VIWulot6YVouVGohjhxVCcSj2qIlUiAG71AYT1gTKrhY2ZteYBMUt6dUPc+595pln5p6Xe2bOvFw/aZ4558yZc665577/z3Ve5oyoKoZhGEY1/kpoAwzDMMaEiaZhGEYNTDQNwzBqYKJpGIZRAxNNwzCMGphoGoZh1KAT0RSRa0XkGRE5LiIHu6jDMAwjBNL2PE0ROQ/4Y+BDwAngW8CNqvpUqxUZhmEEoAtP80rguKo+p6r/B7gH2NdBPYZhGL1zfgdl7gJeTMVPAHuXnbBpk+jWrR1YMlA2b95xLvzaawENMTph8+b18V7u8alTPVQybU7BD1X17WX5uhDNSojIfmA/wJYt8KlPhbKkf6Jo7WLjOJwdRndE0Vq483vsXMcVzAMHL1TJ10Xz/CRwcSq+26etQ1WPqOoeVd2zaVMHVoyE5MflAHD25TeMwdOFaH4LuFxELhWRC4AbgGMd1DMxnImmYYyA1kVTVd8APg18HXgauE9Vv9d2PdPEhTbAaBnrfpkenfRpquqDwINdlD19XEHYMIwhYE8EBSCOXW44jbXUx415mNPFRDMAUeRyw2mcSwaGrJ9zfKTvWXoU3ZgGJpoBqOJpLlj8APsQUBPpdrDPcNoEm6c5Z7KeZplwwnrxTO/bxH7shlGOeZqDJd/r69PzNEaCfQ96xURzkLiNKRmxzG5TZurX15jF52KfT6+YaA4Ol5+a+WEsE9FQrFp/0bkmmgUko4WhrZgdJpqDxBUfKRDIdDyUgHYlmqHKGQLZe2yEx0RzUDiqTGgv8joX4bn/0KZ0zXO/l0PERHOQuGq5lghk1tu0H5xhtINNORoUjrqPTuaNpuc1k23UfXgsmx1RoxDr1+yZ1l930YSdO0XntZ6mWxdfP09z/bE61BHEorwmqu2xeBqo8SOV2ftgo+Wd4uBxVd1Tls9EMxAL4cwKZluDKXU8y6KmvAnoauSJ5tIZAvZZB8WZaA6X5Z4mrOpt5sWriueyuFGPMk+z9J7Y598rzkRz2OR7mmmK0jO5Sn54RR5kWZlF5xrVab15bnSKqyiaNnoegOWrHLnUljlSY+CgqKldVWStv9Mw8jHRHCQus18uWHX6LRfh9L7UmgKP0wS0BewzHB3WPA9Etea5S4WLjvmUGv2RWZEtE+RseasI8JwobZ6XjYbbZ9orzprnY8ctCafjPrVG/2ORaBadXxQ3wWwB++xGh4nmaHDkC+b6eJ0md3af53GWeazpsDXd62Of0/gw0QxAldddFOMy4brnZ0pb4nXW+UFnz6vj+c6SxWcT1AijCaWiKSL/WUROi8h3U2kXichDIvKs32/z6SIinxeR4yLypIi8r0vj54vL7JfkrCBYy/otm4pnXrxuWVPB3hM0LUoHgkTk7wF/Btytqu/2af8eOKOqh0TkILBNVQ+IyHXAZ4DrgL3AYVXdW2bE3AaCyie3l1E3f83SVxC3IpEs27dR99BIi+W6waCJXN/UcG0NBKnqfwfOZJL3AUd9+CjwsVT63ZrwCLBVRHZUttqoTJfCsky46nqdeWXmNd9n15Sfy3VOkKZ9mttV9ZQPvwRs9+FdwIupfCd8mtEyXYvLsoGhIpGr6iXmeZvLugMmJ6RlU42MQXP+qgWoqopI7cmeIrIf2A+wZcuqVswNRx9DCMs8xbx8dftAlzXNxyiUU+paMIpp6mm+vGh2+/1pn34SuDiVb7dP24CqHlHVPaq6Z9OmhlYYvVEkCG15hXkCnU0bOmOw0VidpqJ5DLjJh28C7k+lf8KPol8FvJpqxhst07eolA3erOpp5XmaZeWaUBl9U2X0/MtABPwU8DLwb4HfBe4Dfhp4AbheVc+IiAC/AVwLvA7crKqPlRkxt9FzWD+CXn/0HELP8CsTsrpi1nZ5Q2Hpo5Qjvaap4locPb9RVXeo6ptUdbeq3qWqr6jqNap6uap+UFXP+Lyqqreq6s+o6t+qIphzpP6E9iwOCCsky/oem3jAy/JV6RYwjL6wJ4JGiUv+DkA4qjbL27a1i2sfwudpDB8TzQA0a45ncaxvoi8vs0tBqCKcbfR5jg17Emia2NJwgVitT9OxJpIuJy0cVacMzUVAKy8PZwTH2dJw8yV0X2dVz3NsrGRz3rkj/AwME81JMgRBmmJzvNVrmdDnMjdMNEeNy+yHIZhpmtgzRLFtxR7n1jZjtJhojh6X2Q+XqlORsqIZuruhFRtMLCfD+aENMJJBoeYT3BebT6nww1wMTiyrt/FrZ5fQ1OsMSWvdDCaYk8E8zVHiUvtkK5oA7pwjili3LWhn6lM96orPEPpGQwu3MSzM0xw1joWA5glgNi3rPWbnEXbhXaZJi182XOXcvHIMo29MNAMRx67Ca3zL2eg5Vi+ra5HMUuQNNy1nKOI5FDuMfjDRHAllz6vXFcwpsEw8+xQyE8x5YaI5MpZ7pY50k33oIlomNlWFry0P1jCqYANBgWjyGt9ywVzsq5U3dEz4jCFiohmItAB2M4rdRZmGYZhoBqKJp7mcbBll8XGyivfZq+dqk9kni4lmILr1NB0b+zbbriMMoxFNY7KYaE4WR7tCmVdeUflt1msYw8JEMxBNmuf1m/GO9r1Nx8Zyu6xv3MStdL0YQ8JEMxAhHmFMcJl92+XmpbddV04tA216R8Hus9EVJpojopvXZFQ9Z5XjVfM0Z6iiaZ7m9DDRnB0usze6xDzN6VEqmiJysYh8U0SeEpHvichtPv0iEXlIRJ71+20+XUTk8yJyXESeFJH3dX0Rc6GdqUl1abPONssaB+ZpTo8qj1G+Afyyqj4hIpuBx0XkIeCTwMOqekhEDgIHgQPAh4HL/bYXuNPvjRVprx+0Sjlt1ZVXbldlB6SkeyCKnQnoRCj1NFX1lKo+4cOvAU8Du4B9wFGf7SjwMR/eB9ytCY8AW0VkR+uWGw1xLeVZ1Yau6xgWJpjToVafpohcArwXeBTYrqqn/KGXgO0+vAt4MXXaCZ+WLWu/iDwmIo+9/npNq42GuI7y1sUxR+E0pkFl0RSRtwK/A/ySqv44fUyTl6fXeoG6qh5R1T2qumfTpjpnGvVx1BfMuufUpcuyh4cNCE2HSqIpIm8iEczfUtWv+uSXF81uvz/t008CF6dO3+3TjGC4ns6pU/ZiM4xxUToQJCIC3AU8raq/ljp0DLgJOOT396fSPy0i95AMAL2aasYbK9DsBWx18696Xuiye2Kg80KN7qniaX4A+MfA1SLyHb9dRyKWHxKRZ4EP+jjAg8BzwHHgN4F/1r7Z86SeYDoW4lR/4nfd/EMp2zC6p9TTVNU/BKTg8DU5+RW4dUW7jAzNPcz06ueO8KLlCsIjwrzMWVNlnqYxAOq/hC2bz5EW0o0Cmo13TZ91hcemHE0He4xysjg2imM2nM3fJ46sUI/mNb0N7LLR8+lgojkiwq2M1A/Z96APVjQbYJ7mdDDRHBFhnj3vA7duP2gvE8zTnDkmmsYAcOR3D+SlBWAhks7VFszIOYhJtkXcGDUmmsbAcCzvew1EE+8yc84ibsI5bkw0R8R0+zRdaAPOMfiuASM4JpojYrp9mmW4/moywTRKMNEcEdP1NIfFOuE0ETUymGiOiPl6mhCsCW+iaWQw0TSMZZhoGhnsMUpjJLieq+u5PmM0mKdpGIZRAxNNYyS40AYYBmCiGRQbDR8eNuXIKMNEcySYwPaDa/CopDEvTDRHwhCnG/VrU7t1FXmU5mkaZZhoBiQtOmUCZJ5mu+SKo3PWc2qUYqJpGIZRAxPNgKS9R/MkDWMc2OT2gGSb58uEc4h9msZy4lQXwGIRYlvBffyUepoi8mYR+SMR+Z8i8j0R+RWffqmIPCoix0XkXhG5wKdf6OPH/fFLur2E8VLH0zRP1NH5XE0bBDIqUKV5/hPgalV9D3AFcK2IXAV8DrhDVS8DzgK3+Py3AGd9+h0+n7Ei5mn2gImmUYFS0dSEP/PRN/lNgauBr/j0o8DHfHifj+OPXyMiRe9NN4yauOo564igCaZRkUoDQSJynoh8BzgNPAT8APiRqr7hs5wAdvnwLuBFAH/8VeBtOWXuF5HHROSx119f7SKMOeEq54xjRxRBFPkz3Vrc1sw0mlJpIEhV/xK4QkS2Al8DfnbVilX1CHAEYOdO0VXLmzqJALjQZgwAl9kXE8eZM507l7auP3llm4w5UWvKkar+CPgm8H5gq4gsRHc3cNKHTwIXA/jjW4BXWrHWMFoijpOR7Dhy9npdoxZVRs/f7j1MROSvAh8CniYRz4/7bDcB9/vwMR/HH/+GqponuSLmZRrGMKjSPN8BHBWR80hE9j5VfUBEngLuEZFfBb4N3OXz3wV8SUSOA2eAGzqwezmupTwdU2eepuFYf9Oy8brFrXCuMWuqjJ4/qarvVdW/rarvVtV/59OfU9UrVfUyVf0FVf2JT/8LH7/MH3+u64s4h6P676hqvg6pI5ImqC1igmmswDQeo3Q0E8Em5wTCmueOVm6YCaaxIuMWTceohG8VzNN0rL/hriCfYXTLOEXTMbvfjHmaLhN2ubmGij1zPh3GJ5outAFGGBwbhXM8LKY12fSm8TMu0XShDQjHfJvnjvSNX2lldevPNFpgPKLpQhsQljk3z9eE0jUXzcCCuWieWzN9/IxDNF1oA4yQrPzenpLzTciMOgxfNF1oA4aBNc/T8Tqn18zfEdanOR2GL5oGMO/meYKjy1FzEzOjKiaaVXCMcZbLxHCdlt51E936NKfDsEXThTZgOMy3eb6cxVqZudRomnftaVrzfDoMWzSNGeKWpBcdGz7maU6H4YqmC21AAS5MtfPq03Rs/KCz8bIiaubvCfM0x89wRXPIOIYr6qPAsfEZ8nTasvyGERYTzVVw9PZbnkafpsO59ZPV1+835t+4GUZYTDRHwlSa56tMVK98boM6rK/RqIqJptExLrNvIpzu3HkrPx1UgPU1GlUx0TR6wiV/VxS90vMHOgBkTIdhiqYLbYDRHm4t5AUtvU97j3UFNVvOKoJpzXOjKsMUTWMD0xgIWiMrkEVC2hd9NM9NmKfB+aENMFrGFYQnRt+i2gbWbzoNKnuaInKeiHxbRB7w8UtF5FEROS4i94rIBT79Qh8/7o9f0o3pxgZcaAP6YYyCCe17mu2WZlSljqd5G/A08Nd8/HPAHap6j4j8J+AW4E6/P6uql4nIDT7fP2rRZiONC21ANerq3FiFsWtcaAOMap6miOwGfh74oo8LcDXwFZ/lKPAxH97n4/jj1/j8xgqcm6fpMttIMA1cHRfaAAOo7mn+OvBZYLOPvw34kaq+4eMngF0+vAt4EUBV3xCRV33+H6YLFJH9wH6ALVuamm8sxTGYX1r1een5GYfuebqCcBflG2Ep9TRF5CPAaVV9vM2KVfWIqu5R1T2bNrVZsjFmmopjs7OqlbvYqtZdlt8YN1Wa5x8APioizwP3kDTLDwNbRWThqe4GTvrwSeBiAH98C/BKizbPkqlNOSojLZ6hvMxsrdl4UVoRNno+DUqb56p6O3A7gIhEwL9U1V8Ukd8GPk4ipDcB9/tTjvn4//DHv6Gq2r7pxlTZMGl9GTHEzhG3LKxVSivL4zJ54sg1Es5lZ2TrmCq9zHGteG9Wmad5ALhHRH4V+DZwl0+/C/iSiBwHzgA3rFDHaEl7hotBnLy0WmU2+eI45vGr6glHvY/UZfZ162ojj9EutZ4IUtVYVT/iw8+p6pWqepmq/oKq/sSn/4WPX+aPP9eF4WMijt2G5nUcu9ojyo2bdw1PGwKlnmYEUc/N93RtS1+3kTnHmufTwJ4IGhDL+i0XupBu4i3CaQ80e+xc2LkN4pmtr6vl5wY+8H0ORz0vciGYUQRx3L4txhrZ73lITDRHSPrLk/0iFR2LIwdx+7ZE0UYvOk3XgtmG0DvWPprVS1udOHJEzG/wbyyYaDqG8UsxGhHHrrZw5uVu1Mcc1/M2F/9glnlN61oNJf+QYPWvb7ZFAuu7EbJpbXh7cexG/ZMz0ayCo/Y3M+8Ln/6BD92LcG51L7GPZvni83TUm0vZTt318m/4PrQgQFHkKjUgsv2pVVsry9JWwRWEx4CJZodE0cbBnqZi2YaIrRmRkxZxbtpO7L/G5fUVD2b11Y9ZxdNcftQw6mGi2SHBB0DiZnkj3DnhXHqK/wew+OeQ3kLjQhtQQlteptE/JppTI26nmKxwRjky5KJUXkcyeOH3dagi0EWkhaN5Kc36RlchyvTrNetT7dfmrnAM/59cGhPNkVDJe4vbrTNPKLsgW08dEW1LOPoQn2V1pL12Y9iYaE6JiE6mFQ2ZuYvMqk+ZGfUx0RwZS38kcZ+WDIPYud6fCMq1I66RN2PvEOw3qmMvVoNqKy8EJu93NfRpS02o2785NsHJW1ik7cVGxoirmT/93V+E+/o9mKc5cs71hQ1B2VdklQGhdClrRNXPmsigyphxJcejTDxPOPtgkKLpKF670GWO56V1YdAyZ6CzetN1LKlkCoIJ4GqIlqv0I4lz0qLKdYRgiq2HqTFI0YRiIXI5x7NpRef2TZtzFovKmopg1mUhsBvFM0qF45wz89IWT/csjkW5eYZOegQ+HbbVldplkn2arreTjFWIovVbE86JZ66XGtFMAOOcbTxkm62uMKfRhEmKJsxDA+fqZWZZLpywJp6LrQlxzgZx22vCGaWE7sIYbPO8DVxBOC+fDWBOAxe5Cv2dEW14j+sFM1veoo5ofapr/9UcdXFQ+dUOq5BenKaNQbYqYtmHoE5aNNO4Lst23Ytu8mXouJKeaNoUr0pxf+c6K1LheKX68r3N6mWObdpUVbLdBFOZnTAI0Tx1yjw92PjFyvuvOaUvX9dUE09oU0CrEFIkw9UcvlndFoMQTWONqXyxiujay8yjunhC+GdRs3VHAWwwljHZgaApEkXj9jJDCGYaF7mKc0Ej1g8aRQX5qhCvcG76/LiFsqbSwROWSp6miDwPvAb8JfCGqu4RkYuAe4FLgOeB61X1rIgIcBi4Dngd+KSqPtG+6fNjrE3z0GKZJS2c1Zvui33csjVViAvCeURLj7qV7DCgXvP876vqD1Pxg8DDqnpIRA76+AHgw8DlftsL3On3k8b6ZDcyNLHMo9poe5ooE49bs6UdYr+P1qU5wBWM6Bv1WKVPcx9rn/5RkrtxwKffraoKPCIiW0Vkh6qeWsXQkAxREIc8R3MMYpmmXp9nligTj0uO90W8IcWta+qDiWczqoqmAr8vIgp8QVWPANtTQvgSsN2HdwEvps494dNGI5pDFMkFiyZ6jBu0cI6R1cRzQeT38Uq29EO85FjUkw3jo6po/pyqnhSRvw48JCLfTx9UVfWCWhkR2Q/sr3NOlwxZKI1+aVc8x0pccjzqwYZhUkk0VfWk358Wka8BVwIvL5rdIrIDOO2znwQuTp2+26dlyzwCHAGoK7jzIwbSzd44sx8GY2uWl1G/v3NOxJl4FMCGMJROORKRt4jI5kUY+AfAd4FjwE0+203A/T58DPiEJFwFvDrm/sx+iQu2YZIWyUV4aiJTfZrS3IlLtulQxdPcDnwtmUnE+cB/VdXfE5FvAfeJyC3AC8D1Pv+DJNONjpNMObq5dauNwZD1LqcqMO002edMXJAe9WhDO5SKpqo+B7wnJ/0V4JqcdAVubcW6SRMzxi/M3Om+yR75fdxhHUMiZmy/A3uMslfinHg2rT/K+iCLVj2bWt9lXepNjm9KlArHHdUxFOJMPApgQ3VMNDslDm3ABuoIXhTVe8viHOmn2R7lpMUd1hea2O+jgDYUY6LZGnFoA3JZ1Sucu1dZlf5H2iO/j3uss2/iTDwKYMNGJOmCDGzEaKccxauXkHLlogYKFS2xoSvBc7GNKC8j7GBRHLDuIRE1OelxVd1TlslEszZxu6V1JJrmIQ6DMAIaB6hz6ERVMlUSTWueVyIObUAuLooB63cMyUIUhzURPkqF40A2DI04FY5WKsk8zULifmpp6GkuBNMIx3BEsgpxaANGQGSeZnXi0AZUxsTSaEaUkxb3bMM0mLloxqENqOxdmlga7RNl4nEAG8bHzEQzDm1AbUwsjf6IGONvpG9mIJpxaANqY0JphCPKxOMANgybCQ0ExasXMQBMMMfFuAaDmhKHNqAnqg0EjVQ0407sCImJ5XiZh3DCFH936xm9aMbMY75ZZE/XTIT5iGeaOLQBLVJNNAf63vM4s58eiWdpgjkl5rlgcZTa5sGAPM14SY7I75flGQfWDJ8H8/Q6F8ShDWjIqJrn71T/uqACIsZ7I0wo58q8hXNBHNqAGkxKNMeLi+J1P575Nd+MMlYV1+G/iiMObUBFTDSDshBLE0mjb4YrnjBsATXRDII1xY1l9PWPdNjCmSYObUAKE81eMJE0qpInZFkBzVtqrqnIjkc4YRji2aJoishW4IvAuwEF/gnwDHAvcAnwPHC9qp6V5F2/h0le4/s68ElVfWJ5+eMTzYVYWhPcaMIyQSt7cVvZ921cYpkmDlx/u6J5FPgDVf2iiFwAbAL+FXBGVQ+JyEFgm6oeEJHrgM+QiOZe4LCq7l1e/nhEM+1ZmmAaq9JEFKueZ+JZl5ZEU0S2AN8B3qGpzCLyDBCp6ikR2QHEqvpOEfmCD385m6+4jmGLpjXBja5pKp6rlD8e4p7qaU80ryBRtKeA9wCPA7cBJ1V1q88jwFlV3SoiDwCHVPUP/bGHgQOq+lhxHcMVTRNMo0+y4tZ2S6aon3Qcohp3XH57orkHeAT4gKo+KiKHgR8Dn1mIps93VlW3VRVNEdkP7E9i2/9O0j06HNJ9lkncBbPFmB9di2eVOsdB3GJZ7Ynm3wAeUdVLfPzvAgeBy5hg89zE0hgSXTfbq9Q3DuIWymh3IOgPgH+qqs+IiAPe4g+9khoIukhVPysiPw98mrWBoM+r6pXLyx+GaKaf3jGxNIZEH+I5XsFME69wbruieQXJlKMLgOeAm0lWSLoP+GngBZIpR2d8/+ZvANeSTDm6eVl/ZlJ+GNHMjoSvpbsNccMYAkXiWUfwpjtlKY+4Rl6b3L6UPMHMe4bXBNMYInU9z6pCWDZHdLzEFfKYaOayrM8yROe7YazKKv/ky86dlnAuiAvSTTTXscyzTKdl0w1jLHT5HZ6meC6I/d5E8xzZ5dmStCRuYmlMib5H24vqHCkmmtmmeJKWCttIuTFhQgjosrpHwLxF09azNIw1QrSoRiicY36xWjNcFK+9sCxHMEd4Ew2jdVxcb4qSsZ5Re5pVnwu3fkvDWMMmyhcy7eZ5FcG0KUSGkU+RqNV531DTJewGzDRFs+6qQ9avaRjFtPGEUfbcKnUMlOmJpi3TZhjd0MYK8RN4PHM6omliaRj9MPP+zvGLpomlYfTPjIVz3KJpgmkYYeljIHVg4jlO0TSxNAwDgghqJdE8vw9LqmKCaRjzZWBeZyGDEU0TTMOYD2MRyDwGIZo73vpaaBMMw+iIMQtkHoMQTcMwhsfUxK4tTDQNY4aYIDbHRNMwZoCJZHuYaBpGYEzQxoWJpmGsgAne/CgVTRF5J3BvKukdwL8B7vbplwDPk7z3/Kx/7/lh4DqS955/UlWfaNdsY26ULWXWR12GATWfCBKR84CTwF7gVuCMqh4SkYPANlU9ICLXAZ8hEc29wGFV3bus3J2bd+qn9nyq6TUYE6dMxJoIpwmjkUMnTwRdA/xAVV8QkX1A5NOPkrwH8wCwD7hbEzV+RES2isgOVT1Vsy7DaE3cTCSNtqgrmjcAX/bh7SkhfAnY7sO7gBdT55zwaetEU0T2A/sBtly4paYZhpGPiaPRNZVFU0QuAD4K3J49pqoqIrVW/lDVI/hVOnZu3hl+1RBjcFQVQBNKo0/qvI3yw8ATqvqyj78sIjsA/P60Tz8JXJw6b7dPMwzDGD11muc3stY0BzgG3AQc8vv7U+mfFpF7SAaCXrX+TGOBeYXG2KkkmiLyFuBDQHqI+xBwn4jcArwAXO/THyQZOT9OMuXo5tasNQaJCaExJyqJpqr+OfC2TNorJKPp2bxKMh3JmAEmmMbcGMjK7fIa8ExoO3rmp4AfhjaiR+x6p8/Yr/lvqurbyzIN5THKZ6pMKp0SIvLYnK7Zrnf6zOWa64yeG4ZhzB4TTcMwjBoMRTQ3vvR8+sztmu16p88srnkQA0GGYRhjYSiepmEYxigILpoicq2IPCMix/0Sc6NHRC4WkW+KyFMi8j0Ruc2nXyQiD4nIs36/zaeLiHzefwZPisj7wl5BM0TkPBH5tog84OOXisij/rru9esXICIX+vhxf/ySkHY3xa/g9RUR+b6IPC0i75/yPRaRf+G/z98VkS+LyJunfo/zCCqafn3O/0jyXPu7gBtF5F0hbWqJN4BfVtV3AVcBt/rrOgg8rKqXAw/7OCTXf7nf9gN39m9yK9wGPJ2Kfw64Q1UvA84Ct/j0W4CzPv0On2+MHAZ+T1V/FngPybVP8h6LyC7gnwN7VPXdwHkkq55N/R5vRFWDbcD7ga+n4rcDt4e0qaPrvJ/kMdRngB0+bQfJ/FSALwA3pvKfyzeWjWRhloeBq4EHACGZ6Hx+9l4DXwfe78Pn+3wS+hpqXu8W4E+ydk/1HrO25ONF/p49APzDKd/joi1087xo7c3J4Jsl7wUepf4apGPi14HPAv/Px98G/EhV3/Dx9DWdu15//FUyj+mOgEuBPwX+i++S+KJfo2GS91hVTwL/AfjfJGvjvgo8zrTvcS6hRXPSiMhbgd8BfklVf5w+psm/4ElMXRCRjwCnVfXx0Lb0yPnA+4A7VfW9wJ+z1hQHJnePt5G8leFSYCfwFuDaoEYFIrRoTnbtTRF5E4lg/paqftUnT3UN0g8AHxWR54F7SJroh4GtIrJ4VDd9Teeu1x/fArzSp8EHDSXtAAABQUlEQVQtcAI4oaqP+vhXSER0qvf4g8CfqOqfqur/Bb5Kct+nfI9zCS2a3wIu9yNwF5B0LB8LbNPK+Ddy3gU8raq/ljq0WIMUNq5B+gk/wnoVI1uDVFVvV9XdqnoJyT38hqr+IvBN4OM+W/Z6F5/Dx33+UXlkqvoS8KJ/WyskK349xUTvMUmz/CoR2eS/34vrnew9LiR0pyrJ2pt/DPwA+Neh7Wnpmn6OpFn2JPAdv11H0qfzMPAs8N+Ai3x+IZlF8APgf5GMUAa/jobXHgEP+PA7gD8iWVv1t4ELffqbffy4P/6O0HY3vNYrgMf8ff5dYNuU7zHwK8D3ge8CXwIunPo9ztvsiSDDMIwahG6eG4ZhjAoTTcMwjBqYaBqGYdTARNMwDKMGJpqGYRg1MNE0DMOogYmmYRhGDUw0DcMwavD/AWdHgjb5H7HWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_label, test_output = test_single_image(model, dataset_test, 50)\n",
    "\n",
    "img_show(to_rgb(test_label))\n",
    "\n",
    "img_show(to_rgb(test_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Timit ENV",
   "language": "python",
   "name": "timit_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
